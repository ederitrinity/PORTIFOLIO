# Pipeline de dados - EXTRA√á√ÉO DE DADOS/SCOUTS PARA CRIA√á√ÉO DO DASHBOARD DO CLUBE ATL√âTICO MINEIRO 

O objetivo desse projeto foi criar um Dashboard com dados generalizados, integrando todos os departamentos do clube de futebol profissional para auxilio a √°rea analytics, analistas de desempenho e comiss√£o t√©cnica do Clube atl√©tico Mineiro.  

- **Fonte de Dados:** 
- SofaScore: https://www.sofascore.com/
- WhoScored: https://1xbet.whoscored.com/
- FBRef: https://fbref.com/pt/
- InfoGol: https://www.infogol.net/pt-br
- Footstats: http://www.footstats.com.br/#/

Para este projeto, foram usados as seguintes tecnologias: 

- **Airflow**: Para realizar a tarefa de orquestrar o pipeline;
- **Google Cloud Storage:** Para armazenamento de dados Brutos e Curados;
- **Big Query:** Para armazenamento de dados curados;
- **DataStudio:** Para cria√ß√£o do Dashboard;
- **Google Cloud Dataproc:** Para processamento de dados com Spark;
- **Google Cloud Run:** Para hospedar a API que realize a coleta dos dados na internet;

Os componentes foram organizados na seguinte arquitetura: 

![alt text](./img/arquitetura.png)

Para o Airflow, foi utilizada uma imagem docker que est√° no diret√≥rio /airflow. Entretanto, para  subir o servi√ßo localmente basta executar o comando **make** a partir do diret√≥rio raiz. 

A imagem abaixo ilustra a DAG criada para a orquestra√ß√£o. Existem dois operadores Dummy no inicio e fim para fins de organiza√ß√£o. A task get_op √© a respons√°vel por realizar a chamada da API presente no Cloud Run e assim extrair os dados da fonte. Em seguida, a task create_cluster cria um cluster dataproc para realizar o processamento dos dados com Spark. A pr√≥xima task roda o job de processamento dentro do cluster rec√©m criado e por √∫ltimo √© feita a remo√ß√£o do cluster.

![alt text](./img/dag.png)

A API foi feita utilizando o framework FastAPI e est√° presente no diret√≥rio /api. Para realizar o deploy dessa aplica√ß√£o no Cloud Run basta executar o shell script presente na mesma pasta. Para que o deploy seja de fato realizado, √© preciso estar logado em sua conta na Google Cloud Platform atrav√©s da SDK. Caso ainda n√£o tenha instalado, basta acessar esse [link](https://cloud.google.com/sdk/docs/install-sdk).

O pipeline em PySpark seguiu os seguintes passos: 

A fun√ß√£o main recebe como par√¢metro:
- path_input: Caminho dos dados no GCS gerados pela API coletora. Ex: gs://bucket_name/file_name.
- path_output: Caminho de onde ser√° salvo os dados processados. Ex: gs://bucket_name_2/file_name.
- formato_file_save: Formato de arquivo a ser salvo no path_output. Ex: PARQUET.
- dataset: Dataset no BigQuery onde est√° a tabela.
- tabela_bq: Tabela do BigQuery que ser√° salvo os dados. Ex: dataset.tabela_exemplo

Por √∫ltimo, e ap√≥s a s tranforma√ß√µes nos dados, foi desenvolvido um dashboard usando a ferramenta Data Studio, da GCP. Abaixo est√° o link do relat√≥rio bem como o link para acesso (Nessa parte do projeto pe√ßo desculpas pois a tempos deixei o oficio de data viz a 4 anos atr√°s pra me dedicar a ENGENHARIA DE DADOS, e estou meio enferrujado....kkkk! Mas estou em busca de melhora üôÉ ). 

[Link](https://datastudio.google.com/reporting/acc2d3e0-8b83-44c7-af12-0dc7866b6faa/page/p_pe2dlvrh0c) para o dashboard.

![alt text](./img/dashboard.png)
